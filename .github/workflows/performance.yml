name: Performance Tests

on:
  schedule:
    - cron: '0 2 * * *'  # Run daily at 2 AM
  workflow_dispatch:
    inputs:
      model_id:
        description: 'Model to test (e.g., mlx-community/Mistral-7B-Instruct-v0.3-4bit)'
        required: false
        default: 'mlx-community/Mistral-7B-Instruct-v0.3-4bit'

jobs:
  performance-test:
    name: Performance Benchmark
    runs-on: macos-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd gerdsen_ai_server
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements_dev.txt
    
    - name: Cache models
      uses: actions/cache@v4
      with:
        path: ~/.cache/huggingface
        key: ${{ runner.os }}-models-${{ github.event.inputs.model_id || 'default' }}
        restore-keys: |
          ${{ runner.os }}-models-
    
    - name: Start server
      run: |
        cd gerdsen_ai_server
        python src/main.py &
        SERVER_PID=$!
        echo "SERVER_PID=$SERVER_PID" >> $GITHUB_ENV
        
        # Wait for server to start
        for i in {1..30}; do
          if curl -f http://localhost:8080/api/health/status; then
            echo "Server started successfully"
            break
          fi
          sleep 2
        done
    
    - name: Download and load model
      run: |
        MODEL_ID="${{ github.event.inputs.model_id || 'mlx-community/Mistral-7B-Instruct-v0.3-4bit' }}"
        
        # Download model
        curl -X POST http://localhost:8080/api/models/download \
          -H "Content-Type: application/json" \
          -d "{\"model_id\": \"$MODEL_ID\", \"auto_load\": true}"
        
        # Wait for model to load
        for i in {1..60}; do
          if curl -f "http://localhost:8080/api/models/list" | grep -q "\"status\": \"loaded\""; then
            echo "Model loaded successfully"
            break
          fi
          sleep 5
        done
    
    - name: Run performance benchmarks
      run: |
        cd gerdsen_ai_server
        python -c "
        import requests
        import time
        import json
        import statistics
        
        base_url = 'http://localhost:8080'
        model_id = '${{ github.event.inputs.model_id || 'mlx-community/Mistral-7B-Instruct-v0.3-4bit' }}'
        
        # Performance test configurations
        test_configs = [
            {'prompt': 'Hello, how are you?', 'max_tokens': 50, 'name': 'short_response'},
            {'prompt': 'Write a detailed explanation of machine learning.', 'max_tokens': 200, 'name': 'medium_response'},
            {'prompt': 'Explain the history of artificial intelligence in detail.', 'max_tokens': 500, 'name': 'long_response'}
        ]
        
        results = {}
        
        for config in test_configs:
            print(f'Testing {config[\"name\"]}...')
            latencies = []
            token_rates = []
            
            for i in range(5):  # Run 5 iterations
                start_time = time.time()
                
                response = requests.post(f'{base_url}/v1/chat/completions', json={
                    'model': model_id,
                    'messages': [{'role': 'user', 'content': config['prompt']}],
                    'max_tokens': config['max_tokens'],
                    'temperature': 0.7
                })
                
                end_time = time.time()
                duration = end_time - start_time
                
                if response.status_code == 200:
                    data = response.json()
                    tokens = len(data['choices'][0]['message']['content'].split())
                    token_rate = tokens / duration
                    
                    latencies.append(duration)
                    token_rates.append(token_rate)
                    
                    print(f'  Iteration {i+1}: {duration:.2f}s, {token_rate:.1f} tokens/s')
                else:
                    print(f'  Error in iteration {i+1}: {response.status_code}')
            
            if latencies:
                results[config['name']] = {
                    'avg_latency': statistics.mean(latencies),
                    'min_latency': min(latencies),
                    'max_latency': max(latencies),
                    'avg_token_rate': statistics.mean(token_rates),
                    'min_token_rate': min(token_rates),
                    'max_token_rate': max(token_rates)
                }
        
        # Save results
        with open('performance_results.json', 'w') as f:
            json.dump(results, f, indent=2)
        
        # Print summary
        print('\n=== Performance Summary ===')
        for test_name, metrics in results.items():
            print(f'{test_name}:')
            print(f'  Average latency: {metrics[\"avg_latency\"]:.2f}s')
            print(f'  Average token rate: {metrics[\"avg_token_rate\"]:.1f} tokens/s')
        "
    
    - name: Run memory benchmark
      run: |
        MODEL_ID="${{ github.event.inputs.model_id || 'mlx-community/Mistral-7B-Instruct-v0.3-4bit' }}"
        
        curl -X POST "http://localhost:8080/api/models/benchmark/$MODEL_ID" \
          -H "Content-Type: application/json" \
          -d '{"num_samples": 10, "max_tokens": 100}'
    
    - name: Collect system metrics
      run: |
        # Get hardware info
        curl http://localhost:8080/api/hardware/info > hardware_info.json
        
        # Get performance metrics
        curl http://localhost:8080/api/hardware/metrics > hardware_metrics.json
        
        # Get benchmark history
        MODEL_ID="${{ github.event.inputs.model_id || 'mlx-community/Mistral-7B-Instruct-v0.3-4bit' }}"
        curl "http://localhost:8080/api/models/benchmark/$MODEL_ID/history" > benchmark_history.json
    
    - name: Stop server
      if: always()
      run: |
        if [ ! -z "$SERVER_PID" ]; then
          kill $SERVER_PID || true
        fi
    
    - name: Upload performance results
      uses: actions/upload-artifact@v4
      with:
        name: performance-results-${{ github.run_id }}
        path: |
          performance_results.json
          hardware_info.json
          hardware_metrics.json
          benchmark_history.json
    
    - name: Create performance report
      run: |
        python -c "
        import json
        import os
        
        # Load results
        with open('performance_results.json') as f:
            perf_results = json.load(f)
        
        with open('hardware_info.json') as f:
            hw_info = json.load(f)
        
        # Generate markdown report
        report = f'''# Performance Test Report
        
        **Date**: {os.environ.get('GITHUB_RUN_ID', 'Unknown')}
        **Model**: ${{ github.event.inputs.model_id || 'mlx-community/Mistral-7B-Instruct-v0.3-4bit' }}
        **Hardware**: {hw_info.get('chip_type', 'Unknown')} with {hw_info.get('total_memory_gb', 'Unknown')}GB RAM
        
        ## Results
        
        '''
        
        for test_name, metrics in perf_results.items():
            report += f'''### {test_name.replace('_', ' ').title()}
        - **Average Latency**: {metrics['avg_latency']:.2f}s
        - **Token Rate**: {metrics['avg_token_rate']:.1f} tokens/s
        - **Range**: {metrics['min_token_rate']:.1f} - {metrics['max_token_rate']:.1f} tokens/s
        
        '''
        
        with open('PERFORMANCE_REPORT.md', 'w') as f:
            f.write(report)
        "
    
    - name: Comment performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('PERFORMANCE_REPORT.md', 'utf8');
          
          await github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: report
          });