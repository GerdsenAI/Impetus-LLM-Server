# Impetus LLM Server v0.1.0 Release Notes

## ðŸŽ‰ Introducing Impetus LLM Server

We're excited to announce the first public release of Impetus LLM Server - a high-performance local LLM server specifically optimized for Apple Silicon Macs.

## ðŸš€ Key Highlights

### Lightning Fast on Apple Silicon
- **Optimized for M1/M2/M3/M4**: Leverages MLX framework for maximum performance
- **40-120 tokens/sec**: Depending on your chip and model size
- **<5s model loading**: With memory-mapped I/O
- **<200ms first token**: When models are warmed up

### Developer Friendly
- **OpenAI-compatible API**: Works with VS Code extensions (Cline, Continue, Cursor)
- **5-minute setup**: Quick start guide gets you running fast
- **Real-time dashboard**: Monitor performance and manage models
- **One-click downloads**: Curated list of optimized models

### Production Ready
- **Battle-tested**: Comprehensive test suite with 90%+ coverage
- **Error recovery**: Automatic handling of OOM and thermal issues
- **Service support**: Run as systemd or launchd service
- **Rate limiting**: Built-in production hardening

## ðŸ“¦ What's Included

### Core Features
- âœ… MLX model inference with streaming
- âœ… WebSocket real-time updates
- âœ… KV cache for conversations
- âœ… Model warmup system
- âœ… Memory-mapped loading
- âœ… Comprehensive benchmarking
- âœ… Metal GPU monitoring
- âœ… Thermal management

### Models Supported
- Mistral 7B (recommended starter)
- Llama 3.2 series
- Phi-3 Mini
- DeepSeek Coder
- And 5 more curated models

## ðŸ›  Installation

```bash
# Quick install
curl -sSL https://raw.githubusercontent.com/GerdsenAI/Impetus-LLM-Server/main/install.sh | bash

# Or with pip
pip install impetus-llm-server
```

## ðŸ“Š Performance

| Chip | 7B Model (4-bit) | First Token | Load Time |
|------|------------------|-------------|-----------|
| M1   | 40-60 tok/s     | <200ms      | <5s       |
| M2   | 60-80 tok/s     | <200ms      | <5s       |
| M3   | 80-100 tok/s    | <200ms      | <5s       |
| M4   | 100-120 tok/s   | <200ms      | <5s       |

## ðŸ”® What's Next

We're just getting started! Future releases will include:
- Docker images for easy deployment
- More model format support
- Advanced RAG capabilities
- Multi-modal support
- Fine-tuning interface

## ðŸ™ Thank You

Special thanks to:
- Apple MLX team for the amazing framework
- Early testers who provided invaluable feedback
- The open-source community

## ðŸ“š Resources

- [Documentation](README.md)
- [Quick Start Guide](QUICKSTART.md)
- [API Reference](https://github.com/GerdsenAI/Impetus-LLM-Server/wiki/API-Reference)
- [Report Issues](https://github.com/GerdsenAI/Impetus-LLM-Server/issues)

---

**Happy inferencing!** ðŸš€