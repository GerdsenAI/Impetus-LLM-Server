# Impetus-LLM-Server Environment Configuration Example
# Copy this file to .env and adjust values as needed

# Server settings
PORT=8080
DEBUG=False
LOG_LEVEL=INFO

# Model settings
MODEL_CACHE_DIR=./models
DEFAULT_MODEL=phi-3-mini-4k-instruct-q4.gguf
MAX_CONTEXT_LENGTH=4096
MAX_BATCH_SIZE=8

# Performance settings (0 = auto-detect)
NUM_THREADS=0
MAX_MEMORY_GB=0

# API Authentication
# IMPORTANT: Change these values in production!
# For development, you can use any key starting with 'sk-'
OPENAI_API_KEYS=  # Comma-separated list of valid API keys (e.g., sk-prod-key1,sk-prod-key2)
OPENAI_MASTER_KEY=  # Master key for admin operations (generate a secure random string)
SECRET_KEY=  # Flask secret key (generate a secure random string)

# CORS settings (SECURITY: Restrict in production!)
ENABLE_CORS=True
# Specific allowed origins (no wildcards in production)
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8080,http://127.0.0.1:3000,http://127.0.0.1:8080

# Rate limiting
ENABLE_RATE_LIMIT=True
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_PREMIUM_PER_MINUTE=1000

# Hardware optimization (Apple Silicon)
ENABLE_METAL=True
ENABLE_NEURAL_ENGINE=True
AUTO_OPTIMIZE=True
THERMAL_THROTTLE=True

# Model downloading
HF_TOKEN=  # Optional: Hugging Face token for private models
MODEL_DOWNLOAD_TIMEOUT=3600

# File upload security
UPLOAD_MAX_SIZE_MB=5000  # 5GB max file size
UPLOAD_ALLOWED_EXTENSIONS=.gguf,.safetensors,.mlx,.mlmodel,.pt,.pth,.bin,.onnx,.mlpackage
UPLOAD_DIRECTORY=~/Models  # Base directory for uploads (will be sanitized)

# Monitoring
ENABLE_METRICS=True
METRICS_PORT=9090

# Security settings
ENABLE_AUTH=True  # Set to True for production
REQUIRE_API_KEY=True  # Require API key for all endpoints
ENABLE_ADMIN_ENDPOINTS=False  # Enable admin endpoints (key management, etc.)

# Logging
LOG_FILE=logs/impetus.log
LOG_MAX_BYTES=10485760  # 10MB
LOG_BACKUP_COUNT=5